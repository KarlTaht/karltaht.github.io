---
layout: posts
title: Asynchronous Methods for Deep Reinforcement Learning (A3C)
date: '2020-01-16T08:57:00.001-08:00'
author: Karl Taht
categories: [research-papers, research]
tags:
- Reinforcement Learning
- RL
- A3C
custom_excerpt: "Most importantly, this work presents the asynchronous advantage 
actor-critic framework (A3C), which is currently thought of as the state-of-the-art
for deep reinforcement learning."
except_sepator: <!--more-->
---

This paper presents a framework to perform asynchronous gradient descent for 
optimization of deep nerual networks in the scope of reinforcement learning
problems. Most importantly, this work presents the asynchronous advantage 
actor-critic framework (A3C), which is currently one of the state-of-the-art
frameworks for deep reinforcement learning.

While neural networks are powerful universal function approximators making them
conceptually ideal for approximate reinforcement learning, they suffer from 
an inherit problem of instability and variance making convergence difficult to
achieve. As such, reinforcement learning has traditionally focused on off-policy
learning algorithms which were able to aggregate sufficient data during update
phases. However, this work changes that by providing a framework which reduces
resource requirements (for storing gradients) while maintaining reliability.

The framework centralizes models on a single machine, reducing communication 
costs. Gradient descent can be done in a similar fashion to [Hogwild!]. Unlike
previous works, each simulated environment runs with different exploration policies
and different environments. Since the updates occur online, this reduces the 
likelihood that two parallel learners will have the same bias. The result
is a technique which can be applied to a variety of RL algorithms (one-step SARSA,
n-step Q-Learning, and advantage actor-critic). As a general approach toward
solving RL problems, A3C seems to be one of the most successful in general.

[Hogwild!]: https://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.pdf

### The A3C Algorithm

![A3C Algorithm](/images/a3c_alg.png)

