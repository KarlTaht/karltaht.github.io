---
layout: post
title: Compression of Neural Machine Translation Models via Pruning
date: '2018-03-08T10:19:00.002-08:00'
author: Karl Taht
tags:
- lstm
- neural networks
- pruning
- compression
modified_time: '2018-03-08T10:19:37.440-08:00'
blogger_id: tag:blogger.com,1999:blog-2670908301789336410.post-2523930705751010548
blogger_orig_url: https://researchdoneright.blogspot.com/2018/03/compression-of-neural-machine.html
---

Authors: Abigail See, Minh-Thang Luong, Christopher D. Manning (Stanford)<br />Venue:&nbsp; &nbsp; Arxiv<br /><br />This paper applies pruning techniques to encoder-decoder deep multi-layer recurrent architecture with LSTM as the hidden unit type. The paper tries various pruning types, but finds the most effective to be simply pruning weights of least magnitude overall. While overall the techniques are mostly brought over from pruning techniques used in CNN's and other networks, the papers does make note of interesting artifacts of pruning.<br /><br />Firstly, deeper neurons are more sensitive to pruning that early neurons. In other words, the deeper units are actually of more importance and more sensitive to even low-magnitude weights. Additionally, they find that the sparse models can even out perform the originals, and claim that this is most likely due to the "generalizing" effect that pruning has. They say that while training set accuracy decreases, validation set accuracy actually increases! Additionally, they note that the architecture generated by the sparse model cannot simply be applied by default. In other words, results are far better when you start with the original model, train it completely, and then iteratively prune.<br /><a href="https://nlp.stanford.edu/pubs/see2016compression.pdf"><br /></a><a href="https://nlp.stanford.edu/pubs/see2016compression.pdf">Full Text</a>