<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.17.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Asynchronous Methods for Deep Reinforcement Learning (A3C) - Karl’s Personal Page</title>
<meta name="description" content="This paper presents a framework to perform asynchronous gradient descent for optimization of deep nerual networks in the scope of reinforcement learningproblems. Most importantly, this work presents the asynchronous advantage actor-critic framework (A3C), which is currently one of the state-of-the-artframeworks for deep reinforcement learning.">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Karl's Personal Page">
<meta property="og:title" content="Asynchronous Methods for Deep Reinforcement Learning (A3C)">
<meta property="og:url" content="http://localhost:4000/research-papers/research/a3c/">


  <meta property="og:description" content="This paper presents a framework to perform asynchronous gradient descent for optimization of deep nerual networks in the scope of reinforcement learningproblems. Most importantly, this work presents the asynchronous advantage actor-critic framework (A3C), which is currently one of the state-of-the-artframeworks for deep reinforcement learning.">







  <meta property="article:published_time" content="2020-01-16T09:57:00-07:00">





  

  


<link rel="canonical" href="http://localhost:4000/research-papers/research/a3c/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Karl Taht",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Karl's Personal Page Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--posts">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Karl's Personal Page
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/" >About</a>
            </li><li class="masthead__menu-item">
              <a href="/reality/" >Reality</a>
            </li><li class="masthead__menu-item">
              <a href="/research/" >Research</a>
            </li><li class="masthead__menu-item">
              <a href="/search/" >Search</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name"></h3>
    
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <div class="archive">
    
      <h1 id="page-title" class="page__title">Asynchronous Methods for Deep Reinforcement Learning (A3C)</h1>
    
    <p>This paper presents a framework to perform asynchronous gradient descent for 
optimization of deep nerual networks in the scope of reinforcement learning
problems. Most importantly, this work presents the asynchronous advantage 
actor-critic framework (A3C), which is currently one of the state-of-the-art
frameworks for deep reinforcement learning.</p>

<p>While neural networks are powerful universal function approximators making them
conceptually ideal for approximate reinforcement learning, they suffer from 
an inherit problem of instability and variance making convergence difficult to
achieve. As such, reinforcement learning has traditionally focused on off-policy
learning algorithms which were able to aggregate sufficient data during update
phases. However, this work changes that by providing a framework which reduces
resource requirements (for storing gradients) while maintaining reliability.</p>

<p>The framework centralizes models on a single machine, reducing communication 
costs. Gradient descent can be done in a similar fashion to <a href="https://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.pdf">Hogwild!</a>. Unlike
previous works, each simulated environment runs with different exploration policies
and different environments. Since the updates occur online, this reduces the 
likelihood that two parallel learners will have the same bias. The result
is a technique which can be applied to a variety of RL algorithms (one-step SARSA,
n-step Q-Learning, and advantage actor-critic). As a general approach toward
solving RL problems, A3C seems to be one of the most successful in general.</p>

<h3 id="the-a3c-algorithm">The A3C Algorithm</h3>

<p><img src="/images/a3c_alg.png" alt="A3C Algorithm" /></p>



<ul class="taxonomy__index">
  
  
    <li>
      <a href="#2020">
        <strong>2020</strong> <span class="taxonomy__count">7</span>
      </a>
    </li>
  
    <li>
      <a href="#2019">
        <strong>2019</strong> <span class="taxonomy__count">2</span>
      </a>
    </li>
  
</ul>



  <section id="2020" class="taxonomy__section">
    <h2 class="archive__subtitle">2020</h2>
    <div class="entries-list">
      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/phd/reinforcement-learning/" rel="permalink">Reinforcement Learning Resources
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read

 | <time datetime="2020-01-27T00:00:00-07:00">January 27, 2020</time></p>
    

    <p class="archive__item-excerpt" itemprop="description">Collection of resources for learning about reinforcement learning
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/reality/phd/weekly-recap/" rel="permalink">Week 4 Recap, 2020
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

 | <time datetime="2020-01-26T00:00:00-07:00">January 26, 2020</time></p>
    

    <p class="archive__item-excerpt" itemprop="description">This is going to the first of what I hope are many weekly recaps. Think of it as kind of hybrid between a personal sprint and freeform journaling
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/reality/phd/11_rules/" rel="permalink">Five Axioms for Productivity
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  5 minute read

 | <time datetime="2020-01-21T00:00:00-07:00">January 21, 2020</time></p>
    

    <p class="archive__item-excerpt" itemprop="description">If you’re anything like me, you often find yourself reading or learning about new techniques to improve yourself and your life.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/research-papers/research/a3c/" rel="permalink">Asynchronous Methods for Deep Reinforcement Learning (A3C)
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

 | <time datetime="2020-01-16T09:57:00-07:00">January 16, 2020</time></p>
    

    <p class="archive__item-excerpt" itemprop="description">Most importantly, this work presents the asynchronous advantage actor-critic framework (A3C), which is currently thought of as the state-of-the-art for deep ...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/reality/phd/pomodoro/" rel="permalink">Flow and the Pomodoro Study Technique
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  2 minute read

 | <time datetime="2020-01-15T00:00:00-07:00">January 15, 2020</time></p>
    

    <p class="archive__item-excerpt" itemprop="description">One thing about studying, or really, any kind of working that has limited human interaction faces one key challenge: staying focused.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/reality/phd/rejected/" rel="permalink">Paper Rejection Reflection
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read

 | <time datetime="2020-01-14T00:00:00-07:00">January 14, 2020</time></p>
    

    <p class="archive__item-excerpt" itemprop="description">One thing I’ve found about research in general, there are really two high-level avenues to research.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/research-papers/research/multi-resource-packing-for-cluster/" rel="permalink">Multi-Resource Packing for Cluster Schedulers (Tetris)
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

 | <time datetime="2020-01-08T09:57:00-07:00">January 8, 2020</time></p>
    

    <p class="archive__item-excerpt" itemprop="description">This work presents a resource-aware cluster scheduling scheme which maximizes performance and includes additional parameters to balance fairness requirements.
</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>

  <section id="2019" class="taxonomy__section">
    <h2 class="archive__subtitle">2019</h2>
    <div class="entries-list">
      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/research-papers/research/sparrow-distributed-low-latency/" rel="permalink">Sparrow: Distributed, Low Latency Scheduling
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

 | <time datetime="2019-12-18T10:15:00-07:00">December 18, 2019</time></p>
    

    <p class="archive__item-excerpt" itemprop="description">This work presents Sparrow, a stateless, decentralized scheduler for cluster scheduling. The scheduling component uses two key ideas: batch sampling and late...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/research-papers/research/sosa-self-optimizing-learning-with-self/" rel="permalink">SOSA: Self-Optimizing Learning with Self-Adaptive Control for Hierarchical System-on-chip Management
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  2 minute read

 | <time datetime="2019-12-14T18:19:00-07:00">December 14, 2019</time></p>
    

    <p class="archive__item-excerpt" itemprop="description">This work presents a control theory / reinforcement learning hybrid approach to solve online parameter tuning for SoC’s called SOSA.
</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>


  </div>
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 Karl Taht. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>










  </body>
</html>
