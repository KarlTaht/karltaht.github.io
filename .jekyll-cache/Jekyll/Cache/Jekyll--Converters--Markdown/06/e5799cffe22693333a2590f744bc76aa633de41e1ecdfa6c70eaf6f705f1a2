I"I<p>Authors: Bryan Donyanavard, Tiago Muck, Amir M. Rahmani, Nikil Dutt, Armin Sadighi, Florian Mauer, Andreas Herkersdorf<br />Venue: MICRO 2019<br /><br />This work presents a control theory / reinforcement learning hybrid approach to solve online parameter tuning for SoC‚Äôs called SOSA. While controllers are typically known for being light weight, and RL expensive, the authors build the hierarchy opposite from what you might expect. The RL models, Learning Classifier Tables (LCTs), are used as low-level controllers, and high level supervisor controller uses Supervisory Control Theory (SCT). The SCT controls a high-level system model abstraction, which must be consistent with the low-level system ‚Äúas defined in the Ramadge-Wonham control mechanism‚Äù [1]. This assumption requires further investigation.<br /><br />LCTs are a simpler RL algorithm compared to today‚Äôs deep neural network approaches. They utilize rule-based learning to to target an objective function, which may be multi-variate. The error between the objective value and achieved value (via the action) is used to calculate error, which is used as the reward signal for the agent. Note that the error also requires the max performance as a scaling factor. For IPC this is a reasonable, but may not be the case for other metrics.<br /><br />Evaluation is performed both in simulation and on an FPGA which implements SPARCv8 architecture. In the experiments, the authors tune DVFS and task migration. Load distribution is also a very odd target, particularly because CFS already solves the load balancing in hierarchical fashion, and utilizes the same metric of CPU utilization to perform the balancing. Results indicate that after a few seconds the models converge to an optimum, and can re-converge after a new task is added. However, I felt that the efficacy of the approach was hard to evaluate given the unusual benchmark setup and simulation framework. How much better SOSA is that existing real-world load balancing and firmware governors (which consist of both power saving and performance modes) is unclear.<br /><br />Objectively, I have a few issues with this paper. One one hand, it‚Äôs a novel approach to use both control theory and reinforcement learning in a hybrid solution to solve the knob tuning problem. However, the motivation and results section seem to disagree with themselves. One of the benefits of RL is that lack of a requirement for specifying a particular target, and simply aiming to minimize or maximize a value, yet the use of RL in this case is to minimize error ‚Ä¶ while hitting a particular target. Moreover, if the supervisor is simply specifying ‚Äúthe operating frequency of each core‚Äù, then what exactly are the LCTs doing? Finally, the hardware cost of the approach‚Äìand why evaluation is FPGA/Simulation based‚Äìseems high (~9.6%), about 2% of which is due to the LCT‚Äôs. Is that worth it for 5ms response time? The work is very interesting, but the application of RL seems misplaced. As the number of knobs scale, RL is known to have variance issues. I don‚Äôt see how the LCT‚Äôs will stand up in terms of convergence rate and hardware/computational complexity as the number of knobs scale. And what happens when a system is overloaded and constantly shuffling tasks?<br /><br /><span style="background-color: white; color: #222222; font-family: &quot;arial&quot; , sans-serif; font-size: 13px;">[1] Brandin, Bertil A., and W. Murray Wonham. ‚ÄúSupervisory control of timed discrete-event systems.‚Äù¬†</span><i style="background-color: white; color: #222222; font-family: Arial, sans-serif; font-size: 13px;">IEEE Transactions on Automatic Control</i><span style="background-color: white; color: #222222; font-family: &quot;arial&quot; , sans-serif; font-size: 13px;">¬†39.2 (1994): 329-342.</span><br /><br /><a href="https://dl.acm.org/citation.cfm?id=3358312">Full Text</a></p>
:ET