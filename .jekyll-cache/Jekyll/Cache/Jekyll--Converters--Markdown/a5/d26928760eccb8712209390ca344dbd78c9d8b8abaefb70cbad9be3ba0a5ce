I"ã<p>This paper presents a framework to perform asynchronous gradient descent for 
optimization of deep nerual networks in the scope of reinforcement learning
problems. Most importantly, this work presents the asynchronous advantage 
actor-critic framework (A3C), which is currently thought of as the state-of-the-art
for deep reinforcement learning.</p>

<p>While neural networks are powerful universal function approximators making them
conceptually ideal for approximate reinforcement learning, they suffer from 
an inherit problem of instability and variance making convergence difficult to
achieve. As such, reinforcement learning has traditionally focused on off-policy
learning algorithms which were able to aggregate sufficient data during update
phases. However, this work changes that by providing a framework which reduces
resource requirements (for storing gradients) while maintaining reliability.</p>

<p>The framework centralizes models on a single machine, reducing communication 
costs. Gradient descent can be done in a similar fashin to <a href="https://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.pdf">Hogwild!</a>. Unlike
previous works, each simulated environment runs with different exploration policies
and different environments. Since the updates occur online, this reduces the 
likelihood that two parallel learners will have the same bias. The result
is a technique which can be applied to a variety of RL algorithms (one-step SARSA,
n-step Q-Learning, and advantage actor-critic). The A3C approach is most successful
and represents the current state-of-the-art.</p>

<h3 id="the-a3c-algorithm">The A3C Algorithm</h3>

<p><img src="/images/a3c_alg.png" alt="A3C Algorithm" /></p>

:ET