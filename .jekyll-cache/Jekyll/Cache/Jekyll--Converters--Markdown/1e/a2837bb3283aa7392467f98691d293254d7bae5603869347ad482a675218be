I"a<p>Venue:  ISCA 2019
Authors: Youjie Li, Iou-Jen Li, Yifan Yuan, Deming Chen, Alexander Schwing, Jian Huang</p>

<p>This paper presents iSwitch, an in-switch accelerator which improves the performance of distributed machine learning training – specifically reinforcement learning. While reinforcement learning (RL) models are typically small compared to other DNN applications, they require significantly more training epochs to achieve convergence. As such, distributed training typically suffers from high communication costs because of frequent gradient aggregation and model updates. As such, the authors utilize a NetFPGA board to implement gradient aggregation within the switch itself, significantly reducing both communication and latency. Results are demonstated in for both synchronous and asychronous gradient updates, as well as with two baselines: a central parameter server (PS) and AllReduce-based training (AR).</p>

<p>Latency improves because gradient aggregation can happen as soon as gradients arrive at the switch. This happens asynchronously since environments, episodes, and overall experimental noise cause slight difference in gradient arrival time. The latency improvement also means that in the asynchronous scenario, training converges faster compared to the baseline since models are not so out of date with each other. Additionally, iSwitch is implemented such that gradient updates happen on each individual worker node, rather than at a centralized parameter server. Again, since communication is the dominant factor, it’s worth duplicating this work on individual worker nodes rather than performing the communication. Note that without gradient aggregation at the switch, you can’t perform worker level model updates with having a separate method for aggregation.</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/3307650.3322259">Paper</a>
 / <a href="https://yifanyuan3.github.io/files/iswitch.pdf">Slides</a></p>

<p><em>Personal take:</em> To the best of my knowledge, this is the first paper which proposes a switch-level optimization to improve machine learning training. However, the first paper to propose asynchronous gradient updates for RL (see my <a href="https://karltaht.github.io/research-papers/research/a3c/">A3C Post</a>) mentions the key benefit is that all of this can be done a single machine, eliminating the majority of communication costs. In essense, state-of-the-art RL isn’t trained in a distributed fashion because 1) the models are small enough and 2) the communication is the bottleneck, so keeping everything on one machine is bonus. So while it’s clear the authors chose reinforcement learning because of it’s unique bottleneck of communication overheads, but I think this paper is equally important for scaling distributed training, just that RL was chosen because the benefits are more significant than other ML applications.</p>

:ET